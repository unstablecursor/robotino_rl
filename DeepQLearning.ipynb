{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "import rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "frames = [] # for animation\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10x10env, 0=Wall,1=Free,2=Start,3=Goal\n",
    "env1 = np.zeros((10,10))\n",
    "env1[4,:] =1\n",
    "env1[4,0] = 2\n",
    "env1[4,-1] = 3\n",
    "print(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q.shape)\n",
    "print(Q.shape[1])\n",
    "print(np.zeros(Q.shape[1]))\n",
    "test = np.append(Q,[np.zeros(Q.shape[1])], axis=0)\n",
    "print(test.shape)\n",
    "\n",
    "a=np.array([0,1,1,4,4])\n",
    "b=np.array([3])\n",
    "print(np.isin(b, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpaceState(env):\n",
    "    numberOfStates=np.count_nonzero(env)\n",
    "    stateSpace = np.zeros((numberOfStates,env.shape[0],env.shape[1]))\n",
    "    x=0\n",
    "    for i in range(0,env.shape[0]):\n",
    "        for j in range(0,env.shape[1]):\n",
    "            state=np.copy(env)     \n",
    "            if(state[j,i]!=0):\n",
    "                state[j,i]=4               \n",
    "                stateSpace[x]=state\n",
    "                x=x+1\n",
    "    return stateSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpaceState(env,row,col):\n",
    "    stateSpace = np.zeros((1,env.shape[0],env.shape[1]))\n",
    "    tmp = np.copy(env)\n",
    "    tmp[row,col]=4\n",
    "    stateSpace[0] = tmp\n",
    "    return stateSpace\n",
    "\n",
    "def addNewState(state, stateSpace,Q):\n",
    "    stateSpace=np.append(stateSpace,[state], axis=0) #Grow State Space\n",
    "    Q=np.append(Q,[np.zeros(Q.shape[1])], axis=0) #Grow QTable\n",
    "    return stateSpace, Q\n",
    "    \n",
    "def getState(state, stateSpace, Q):\n",
    "    x = 0\n",
    "    for i in stateSpace:\n",
    "        if((state==i).all()):\n",
    "            return stateSpace[x],x, stateSpace, Q\n",
    "        x=x+1\n",
    "    stateSpace, Q = addNewState(state,stateSpace, Q)\n",
    "    return state, stateSpace.shape[0]-1, stateSpace, Q\n",
    "\n",
    "def whichAction(a,stateSpace,state, rewards, env, Q):\n",
    "    newState = np.copy(state)\n",
    "    tmpSpace=np.copy(state)\n",
    "    pos=np.argwhere(state==4)[0]\n",
    "    row=pos[0]\n",
    "    col=pos[1]\n",
    "    tmpSpace[row,col]=5 #Mark visited\n",
    "    returnReward=0\n",
    "    goal = False\n",
    "    if(a=='N'):\n",
    "        if(row>0 and state[row-1,col]!=0):\n",
    "            if(env[row-1,col]==3):\n",
    "                goal=True\n",
    "            if(tmpSpace[row-1,col]==5):\n",
    "                returnReward=rewards[3] #Punish revisit\n",
    "            tmpSpace[row-1,col]=4\n",
    "            newState,x, stateSpace,Q=getState(tmpSpace,stateSpace,Q)\n",
    "            returnReward=rewards[2]+returnReward\n",
    "            #print('Move North')\n",
    "        else:\n",
    "            returnReward=rewards[1]\n",
    "            #print('Hit Wall North')\n",
    "    elif(a=='E'):\n",
    "        if(col< state.shape[1]-1 and state[row,col+1]!=0):\n",
    "            if(env[row,col+1]==3):\n",
    "                goal=True\n",
    "            if(tmpSpace[row,col+1]==5):\n",
    "                returnReward=rewards[3] #Punish revisit\n",
    "            tmpSpace[row,col+1]=4\n",
    "            newState,x, stateSpace,Q=getState(tmpSpace,stateSpace,Q)\n",
    "            returnReward=rewards[2]+returnReward\n",
    "            #print('Move East')\n",
    "        else:\n",
    "            returnReward=rewards[1]\n",
    "            #print('Hit Wall East')    \n",
    "    elif(a=='S'):\n",
    "        if(row < state.shape[0]-1 and state[row+1,col]!=0):\n",
    "            if(env[row+1,col]==3):\n",
    "                goal=True\n",
    "            if(tmpSpace[row+1,col]==5):\n",
    "                returnReward=rewards[3] #Punish revisit\n",
    "            tmpSpace[row+1,col]=4\n",
    "            newState,x, stateSpace,Q=getState(tmpSpace,stateSpace,Q)\n",
    "            returnReward=rewards[2]+returnReward\n",
    "           # print('Move South')\n",
    "        else:\n",
    "            returnReward=rewards[1]\n",
    "            #print('Hit Wall South')  \n",
    "    elif(a=='W'):\n",
    "        if(col>0 and state[row,col-1]!=0):\n",
    "            if(env[row,col-1]==3):\n",
    "                goal=True\n",
    "            if(tmpSpace[row,col-1]==5):\n",
    "                returnReward=rewards[3] #Punish revisit\n",
    "            tmpSpace[row,col-1]=4\n",
    "            newState,x,stateSpace,Q=getState(tmpSpace,stateSpace,Q)\n",
    "            returnReward=rewards[2]+returnReward\n",
    "            #print('Move West')\n",
    "        else:\n",
    "            returnReward=rewards[1]\n",
    "            #print('Hit Wall West')\n",
    "    if(goal):\n",
    "        returnReward = returnReward+rewards[0]\n",
    "    return newState, returnReward, goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed scene fields don't change, get possible StateSpaces\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "possibleStates=10\n",
    "stateSpace = np.zeros((1,10,10))\n",
    "state = np.copy(env1)\n",
    "state[4,0]=4\n",
    "stateSpace[0]=state\n",
    "#for i in range(0,10):\n",
    " #   state = np.copy(env1)\n",
    "  #  state[4,i]=4\n",
    "   # stateSpace[i]=state\n",
    "print(stateSpace)\n",
    "plt.imshow(stateSpace[0], cmap='cool')\n",
    "plt.show()\n",
    "actions = np.array(['N','E','S','W'])\n",
    "Q=np.zeros((stateSpace.shape[0], actions.shape[0]))\n",
    "rewards = np.array([+50,-5,-1, -5]) #Reach Goal, Hit Wall, Make Move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "epsilon = 0.2\n",
    "learningRate = 0.8\n",
    "gamma = 0.8\n",
    "done = False\n",
    "currentState = stateSpace[0]\n",
    "counter = 0\n",
    "\n",
    "\n",
    "while not done:    \n",
    "    if random.uniform(0,1)<epsilon:\n",
    "        actionId = random.randrange(actions.shape[0]) #Explore action\n",
    "        a = actions[0] #Choose Action\n",
    "    else:\n",
    "        state,stateId, stateSpace, Q=getState(currentState, stateSpace, Q)\n",
    "        actionId = np.argmax(Q[stateId, :])\n",
    "        a = actions[actionId]\n",
    "    oldState=np.copy(currentState)\n",
    "    currentState,myReward, done = whichAction(a,stateSpace,currentState, rewards, env1, Q)\n",
    "    state,stateId, stateSpace, Q=getState(currentState, stateSpace, Q)\n",
    "    oldState,oldStateId,stateSpace, Q=getState(oldState, stateSpace, Q)\n",
    "    Q[oldStateId,actionId] = Q[oldStateId,actionId]+learningRate*(myReward+gamma*np.max(Q[stateId, :])-Q[oldStateId,actionId])\n",
    "    counter = counter + 1\n",
    "    plt.imshow(currentState, cmap='cool')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#print(Q)\n",
    "print(\"Counter\", counter)\n",
    "#plt.imshow(currentState, cmap='cool')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment 2\n",
    "#10x10env, 0=Wall,1=Free,2=Start,3=Goal\n",
    "env2 = np.zeros((10,10))\n",
    "env2[0,:] =1\n",
    "env2[6,0:4] =1\n",
    "env2[8,0:4] =1\n",
    "env2[-1,5:] =1\n",
    "env2[1:,4] =1\n",
    "env2[0,0] = 2\n",
    "env2[-1,-1] = 3\n",
    "#Fixed scene fields don't change, get possible StateSpaces\n",
    "possibleStates=10\n",
    "stateSpace2 = getSpaceState(env2,0,0)\n",
    "\n",
    "actions2 = np.array(['N','E','S','W'])\n",
    "Q2=np.zeros((stateSpace2.shape[0], actions2.shape[0]))\n",
    "rewards2 = np.array([1000,-20,-0.5, -10]) #Reach Goal, Hit Wall, Make Move, Step on already visited\n",
    "\n",
    "plt.imshow(stateSpace2[0], cmap='cool')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2=np.zeros((stateSpace2.shape[0], actions2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "%matplotlib inline\n",
    "import random\n",
    "from time import time\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from matplotlib import animation, rc\n",
    "counters= np.array([]).astype('int')\n",
    "epsilon = 0.6\n",
    "learningRate = 0.6\n",
    "gamme = 0.9\n",
    "counter=0\n",
    "max_episode=500\n",
    "max_count=100\n",
    "episode=0\n",
    "start_time = time()\n",
    "f = IntProgress(min=0, max=max_episode)\n",
    "display(f)\n",
    "while episode < max_episode:\n",
    "    ims = np.array([])\n",
    "    done = False\n",
    "    currentState = stateSpace2[0]\n",
    "    counter = 0\n",
    "    #print(\"Episode\", episode)\n",
    "    #One Episode\n",
    "    while not done and counter < max_count:   \n",
    "        f.value=episode\n",
    "        if random.uniform(0,1)<epsilon:\n",
    "            actionId = random.randrange(4) #Explore action\n",
    "            a = actions2[0] #Choose Action\n",
    "        else:\n",
    "            state,stateId, stateSpace2, Q2=getState(currentState, stateSpace2, Q2)\n",
    "            actionId = np.argmax(Q2[stateId, :])\n",
    "            a = actions2[actionId]\n",
    "        oldState=np.copy(currentState)\n",
    "        currentState,myReward, done = whichAction(a,stateSpace2,currentState, rewards2, env2, Q2)\n",
    "        state,stateId,stateSpace2, Q2=getState(currentState, stateSpace2, Q2)\n",
    "        oldState,oldStateId,stateSpace2, Q2=getState(oldState, stateSpace2, Q2)\n",
    "        Q2[oldStateId,actionId] = Q2[oldStateId,actionId]+learningRate*(myReward+gamma*np.max(Q2[stateId, :])-Q2[oldStateId,actionId])\n",
    "        im =  plt.imshow(state, cmap='cool')\n",
    "        ims = np.append(ims,[state])\n",
    "        #plt.show()\n",
    "        counter = counter + 1\n",
    "    episode=episode+1\n",
    "    #print(\"Steps: \", counter)\n",
    "    counters=np.append(counters,counter)\n",
    "    if(episode==5):\n",
    "        epsilon=0.3\n",
    "        Q2=np.zeros((stateSpace2.shape[0], actions2.shape[0])) #WarmUp Ended\n",
    "    if(episode == 250):\n",
    "        epsilon=0.2\n",
    "    if(episode == 300):\n",
    "        epsilon=0.05\n",
    "    if(episode == 400):\n",
    "        epsilon=0.01\n",
    "    if(episode == 450):\n",
    "        epsilon=0.00\n",
    "    \n",
    "\n",
    "end_time = time()\n",
    "seconds_elapsed = end_time - start_time\n",
    "print(\"Seconds\", seconds_elapsed)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "t = np.arange(0, episode, 1)\n",
    "ax.plot(t, counters)\n",
    "\n",
    "ax.set(xlabel='episodes', ylabel='steps',\n",
    "       title='Steps per episode')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "done=False\n",
    " #Grow State Space\n",
    "states = np.zeros([1,10,10])\n",
    "states[0]=stateSpace2[0]\n",
    "currentState = stateSpace2[0]\n",
    "ims = []\n",
    "fig = plt.figure()\n",
    "while not done:   \n",
    "    state,stateId, stateSpace2, Q2=getState(currentState, stateSpace2, Q2)\n",
    "    actionId = np.argmax(Q2[stateId, :])\n",
    "    a = actions2[actionId]\n",
    "    \n",
    "    currentState,myReward, done = whichAction(a,stateSpace2,currentState, rewards2, env2, Q2)\n",
    "    #plt.imshow(currentState, cmap='cool')\n",
    "    #plt.show()\n",
    "    states=np.append(states,[currentState], axis=0)\n",
    "    im = plt.imshow(currentState, cmap='cool', animated=True)#\n",
    "    ims.append([im])\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=100, blit=True,\n",
    "                                repeat_delay=5)\n",
    "plt.show()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "## Environment 3\n",
    "#10x10env, 0=Wall,1=Free,2=Start,3=Goal\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "env3 = np.zeros((20,20))\n",
    "env3[0,0:7]=1\n",
    "env3[0,8:20]=1\n",
    "\n",
    "env3[1,0]=1\n",
    "env3[1,6]=1\n",
    "env3[1,8]=1\n",
    "env3[1,11]=1\n",
    "env3[1,19]=1\n",
    "\n",
    "env3[2,0:4]=1\n",
    "env3[2,6]=1\n",
    "env3[2,8]=1\n",
    "env3[2,10:13]=1\n",
    "env3[2,16:17]=1\n",
    "env3[2,19]=1\n",
    "\n",
    "env3[3,3]=1\n",
    "env3[3,6]=1\n",
    "env3[3,8]=1\n",
    "env3[3,10]=1\n",
    "env3[3,12:18]=1\n",
    "env3[3,19]=1\n",
    "\n",
    "env3[4,1:4]=1\n",
    "env3[4,6]=1\n",
    "env3[4,8]=1\n",
    "env3[4,17]=1\n",
    "env3[4,19]=1\n",
    "\n",
    "env3[5,1]=1\n",
    "env3[5,3]=1\n",
    "env3[5,6:18]=1\n",
    "env3[5,19]=1\n",
    "\n",
    "env3[6,1]=1\n",
    "env3[6,10]=1\n",
    "env3[6,19]=1\n",
    "\n",
    "env3[7,0:2]=1\n",
    "env3[7,4:6]=1\n",
    "env3[7,8:18]=1\n",
    "env3[7,19]=1\n",
    "\n",
    "env3[8,4:6]=1\n",
    "env3[8,8]=1\n",
    "env3[8,12]=1\n",
    "env3[8,10]=1\n",
    "env3[8,17]=1\n",
    "env3[8,19]=1\n",
    "\n",
    "env3[9,0]=1\n",
    "env3[9,5]=1\n",
    "env3[9,8:16]=1\n",
    "env3[9,17]=1\n",
    "env3[9,19]=1\n",
    "\n",
    "env3[10,0:6]=1\n",
    "env3[10,8]=1\n",
    "env3[10,10]=1\n",
    "env3[10,12]=1\n",
    "env3[10,17]=1\n",
    "env3[10,19]=1\n",
    "\n",
    "env3[11,0]=1\n",
    "env3[11,5:18]=1\n",
    "env3[11,19]=1\n",
    "\n",
    "env3[12,0]=1\n",
    "env3[12,8]=1\n",
    "env3[12,13]=1\n",
    "env3[12,17]=1\n",
    "env3[12,19]=1\n",
    "\n",
    "env3[13,0]=1\n",
    "env3[13,2:7]=1\n",
    "env3[13,8]=1\n",
    "env3[13,10]=1\n",
    "env3[13,13]=1\n",
    "env3[13,15:18]=1\n",
    "env3[13,19]=1\n",
    "\n",
    "env3[14,0:3]=1\n",
    "env3[14,6:15]=1\n",
    "env3[14,17]=1\n",
    "env3[14,19]=1\n",
    "\n",
    "env3[15,1]=1\n",
    "env3[15,6]=1\n",
    "env3[15,8]=1\n",
    "env3[15,10]=1\n",
    "env3[15,16:18]=1\n",
    "env3[15,19]=1\n",
    "\n",
    "env3[16,1:7]=1\n",
    "env3[16,8]=1\n",
    "env3[16,10:15]=1\n",
    "env3[16,16:18]=1\n",
    "env3[16,19]=1\n",
    "\n",
    "env3[17,19]=1\n",
    "\n",
    "env3[18,0:20]=1\n",
    "\n",
    "env3[19,0]=1\n",
    "env3[19,2]=1\n",
    "env3[19,4]=1\n",
    "env3[19,6]=1\n",
    "env3[19,8]=1\n",
    "env3[19,10]=1\n",
    "env3[19,12]=1\n",
    "env3[19,14]=1\n",
    "env3[19,16]=1\n",
    "env3[19,18]=1\n",
    "env3[9,10] = 2\n",
    "env3[19,0] = 3\n",
    "#Fixed scene fields don't change, get possible StateSpaces\n",
    "\n",
    "stateSpace3 = getSpaceState(env3,10,10)\n",
    "\n",
    "actions3 = np.array(['N','E','S','W'])\n",
    "Q3=np.zeros((stateSpace3.shape[0], actions3.shape[0]))\n",
    "rewards3 = np.array([10000,-200,-0.5, -20]) #Reach Goal, Hit Wall, Make Move, Move onto already stepped\n",
    "\n",
    "plt.imshow(env3, cmap='rainbow')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "%matplotlib inline\n",
    "import random\n",
    "from time import time\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from matplotlib import animation, rc\n",
    "counters= np.array([]).astype('int')\n",
    "epsilon = 0.9\n",
    "learningRate = 0.7\n",
    "gamme = 0.9\n",
    "counter=0\n",
    "max_episode=20\n",
    "max_count=10000000000\n",
    "episode=0\n",
    "start_time = time()\n",
    "f = IntProgress(min=0, max=max_episode)\n",
    "display(f)\n",
    "while episode < max_episode:\n",
    "    ims = np.array([])\n",
    "    done = False\n",
    "    currentState = stateSpace3[0]\n",
    "    counter = 0\n",
    "    #print(\"Episode\", episode)\n",
    "    #One Episode\n",
    "    while not done and counter < max_count:   \n",
    "        f.value=episode\n",
    "        if random.uniform(0,1)<epsilon:\n",
    "            actionId = random.randrange(4) #Explore action\n",
    "            a = actions3[0] #Choose Action\n",
    "        else:\n",
    "            state,stateId, stateSpace3, Q3=getState(currentState, stateSpace3, Q3)\n",
    "            actionId = np.argmax(Q3[stateId, :])\n",
    "            a = actions3[actionId]\n",
    "        oldState=np.copy(currentState)\n",
    "        currentState,myReward, done = whichAction(a,stateSpace3,currentState, rewards3, env3, Q3)\n",
    "        if(done):\n",
    "            print(\"DONE in Count:\", counter)\n",
    "        state,stateId,stateSpace3, Q3=getState(currentState, stateSpace3, Q3)\n",
    "        oldState,oldStateId,stateSpace3, Q3=getState(oldState, stateSpace3, Q3)\n",
    "        Q3[oldStateId,actionId] = Q3[oldStateId,actionId]+learningRate*(myReward+gamma*np.max(Q3[stateId, :])-Q3[oldStateId,actionId])\n",
    "        im =  plt.imshow(state, cmap='cool')\n",
    "        ims = np.append(ims,[state])\n",
    "        #plt.show()\n",
    "        counter = counter + 1\n",
    "    episode=episode+1\n",
    "    #print(\"Steps: \", counter)\n",
    "    counters=np.append(counters,counter)\n",
    "    if(episode==1):\n",
    "        epsilon=0.3\n",
    "        #Q3=np.zeros((stateSpace3.shape[0], actions3.shape[0])) #WarmUp Ended\n",
    "        max_count=700\n",
    "    #if(episode == 300):\n",
    "       # epsilon=0.2\n",
    "    #if(episode == 800):\n",
    "      #  epsilon=0.05\n",
    "   # if(episode == 900):\n",
    "      #  epsilon=0.01\n",
    "    #if(episode == 950):\n",
    "      #  epsilon=0.00\n",
    "    \n",
    "\n",
    "end_time = time()\n",
    "seconds_elapsed = end_time - start_time\n",
    "print(\"Seconds\", seconds_elapsed)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "t = np.arange(0, episode, 1)\n",
    "ax.plot(t, counters)\n",
    "\n",
    "ax.set(xlabel='episodes', ylabel='steps',\n",
    "       title='Steps per episode')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('test2.txt', Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "done=False\n",
    " #Grow State Space\n",
    "states = np.zeros([1,20,20])\n",
    "states[0]=stateSpace3[0]\n",
    "currentState = stateSpace3[0]\n",
    "ims = []\n",
    "fig = plt.figure()\n",
    "while not done:   \n",
    "    state,stateId, stateSpace3, Q3=getState(currentState, stateSpace3, Q3)\n",
    "    actionId = np.argmax(Q3[stateId, :])\n",
    "    a = actions3[actionId]\n",
    "    \n",
    "    currentState,myReward, done = whichAction(a,stateSpace3,currentState, rewards3, env3, Q3)\n",
    "    #plt.imshow(currentState, cmap='cool')\n",
    "    #plt.show()\n",
    "    states=np.append(states,[currentState], axis=0)\n",
    "    im = plt.imshow(currentState, cmap='cool', animated=True)#\n",
    "    ims.append([im])\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=100, blit=True,\n",
    "                                repeat_delay=5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trying DL\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0), target=(19,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        #self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.target = target\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        print(self.free_cells)\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "\n",
    "\n",
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img\n",
    "\n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.4 : epsilon = 0.1\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (1, 0), (1, 6), (1, 8), (1, 11), (1, 19), (2, 0), (2, 1), (2, 2), (2, 3), (2, 6), (2, 8), (2, 10), (2, 11), (2, 12), (2, 16), (2, 19), (3, 3), (3, 6), (3, 8), (3, 10), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 19), (4, 1), (4, 2), (4, 3), (4, 6), (4, 8), (4, 17), (4, 19), (5, 1), (5, 3), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 19), (6, 1), (6, 10), (6, 19), (7, 0), (7, 1), (7, 4), (7, 5), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 19), (8, 4), (8, 5), (8, 8), (8, 10), (8, 12), (8, 17), (8, 19), (9, 0), (9, 5), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 17), (9, 19), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 8), (10, 10), (10, 12), (10, 17), (10, 19), (11, 0), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 19), (12, 0), (12, 8), (12, 13), (12, 17), (12, 19), (13, 0), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 8), (13, 10), (13, 13), (13, 15), (13, 16), (13, 17), (13, 19), (14, 0), (14, 1), (14, 2), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 17), (14, 19), (15, 1), (15, 6), (15, 8), (15, 10), (15, 16), (15, 17), (15, 19), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (16, 6), (16, 8), (16, 10), (16, 11), (16, 12), (16, 13), (16, 14), (16, 16), (16, 17), (16, 19), (17, 19), (18, 0), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (18, 6), (18, 7), (18, 8), (18, 9), (18, 10), (18, 11), (18, 12), (18, 13), (18, 14), (18, 15), (18, 16), (18, 17), (18, 18), (18, 19), (19, 0), (19, 2), (19, 4), (19, 6), (19, 8), (19, 10), (19, 12), (19, 14), (19, 16), (19, 18)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2490cc609d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "\n",
    "\n",
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "maze2 = np.copy(env3)\n",
    "maze2[9,10]=1\n",
    "maze2[19,0]=1\n",
    "qmaze = Qmaze(maze2, rat=(9,10),target=(19,0))\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (1, 0), (1, 6), (1, 8), (1, 11), (1, 19), (2, 0), (2, 1), (2, 2), (2, 3), (2, 6), (2, 8), (2, 10), (2, 11), (2, 12), (2, 16), (2, 19), (3, 3), (3, 6), (3, 8), (3, 10), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 19), (4, 1), (4, 2), (4, 3), (4, 6), (4, 8), (4, 17), (4, 19), (5, 1), (5, 3), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 19), (6, 1), (6, 10), (6, 19), (7, 0), (7, 1), (7, 4), (7, 5), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 19), (8, 4), (8, 5), (8, 8), (8, 10), (8, 12), (8, 17), (8, 19), (9, 0), (9, 5), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 17), (9, 19), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 8), (10, 10), (10, 12), (10, 17), (10, 19), (11, 0), (11, 5), (11, 6), (11, 7), (11, 8), (11, 9), (11, 10), (11, 11), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 19), (12, 0), (12, 8), (12, 13), (12, 17), (12, 19), (13, 0), (13, 2), (13, 3), (13, 4), (13, 5), (13, 6), (13, 8), (13, 10), (13, 13), (13, 15), (13, 16), (13, 17), (13, 19), (14, 0), (14, 1), (14, 2), (14, 6), (14, 7), (14, 8), (14, 9), (14, 10), (14, 11), (14, 12), (14, 13), (14, 14), (14, 17), (14, 19), (15, 1), (15, 6), (15, 8), (15, 10), (15, 16), (15, 17), (15, 19), (16, 1), (16, 2), (16, 3), (16, 4), (16, 5), (16, 6), (16, 8), (16, 10), (16, 11), (16, 12), (16, 13), (16, 14), (16, 16), (16, 17), (16, 19), (17, 19), (18, 0), (18, 1), (18, 2), (18, 3), (18, 4), (18, 5), (18, 6), (18, 7), (18, 8), (18, 9), (18, 10), (18, 11), (18, 12), (18, 13), (18, 14), (18, 15), (18, 16), (18, 17), (18, 18), (18, 19), (19, 0), (19, 2), (19, 4), (19, 6), (19, 8), (19, 10), (19, 12), (19, 14), (19, 16), (19, 18)]\n",
      "Epoch: 000/14999 | Loss: 0.0033 | Episodes: 861 | Win count: 0 | Win rate: 0.000 | time: 40.41 minutes\n",
      "Epoch: 001/14999 | Loss: 0.0014 | Episodes: 870 | Win count: 0 | Win rate: 0.000 | time: 1.26 hours\n",
      "Epoch: 002/14999 | Loss: 0.0001 | Episodes: 829 | Win count: 0 | Win rate: 0.000 | time: 1.88 hours\n",
      "Epoch: 003/14999 | Loss: 0.0011 | Episodes: 869 | Win count: 0 | Win rate: 0.000 | time: 2.62 hours\n",
      "Epoch: 004/14999 | Loss: 0.0015 | Episodes: 853 | Win count: 0 | Win rate: 0.000 | time: 3.35 hours\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-26491150d7d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaze2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mqtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaze2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-a1a57c562a15>\u001b[0m in \u001b[0;36mqtrain\u001b[1;34m(model, maze, **opt)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;31m# Train neural network model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             h = model.fit(\n\u001b[0;32m     67\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-3de0a29cbe6a>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, data_size)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# There should be no target values for actions not taken.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;31m# Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mQ_sa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvstate_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-3de0a29cbe6a>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, envstate)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1625\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1626\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 705\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2968\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2970\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2971\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   2972\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learningRate=0.85\n",
    "epsilon = 0.4\n",
    "\n",
    "\n",
    "model = build_model(maze2, learningRate)\n",
    "qtrain(model, maze2, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
